{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implementing a MapReduce Algorithm to Calculate the Average Value for Each Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simplified version implementation(directly run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Map Function\n",
    "The Map function will read the key-value pairs and output intermediate key-value pairs where the key is unchanged, and the value is a tuple containing the original value and a count of 1. This helps in the subsequent aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(data):\n",
    "    \"\"\"Process each key-value pair and emit (key, (value, count))\"\"\"\n",
    "    intermediate = []\n",
    "    for key, value in data:\n",
    "        intermediate.append((key, (value, 1)))\n",
    "    return intermediate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Shuffle and Sort\n",
    "This step is typically handled by the MapReduce framework, where it organizes the data from the Map function to bring together all values associated with the same key to the same reducer. In our implementation, we'll simulate this with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_sort(intermediate):\n",
    "    \"\"\"Organize data by keys\"\"\"\n",
    "    grouped = {}\n",
    "    for key, value in intermediate:\n",
    "        if key in grouped:\n",
    "            grouped[key].append(value)\n",
    "        else:\n",
    "            grouped[key] = [value]\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reduce Function\n",
    "The Reduce function will take the organized data from the shuffle and sort phase, sum the values and counts for each key, and then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_function(grouped):\n",
    "    \"\"\"Aggregate sums and counts and compute the average for each key\"\"\"\n",
    "    result = {}\n",
    "    for key, values in grouped.items():\n",
    "        total_sum = sum(value[0] for value in values)\n",
    "        total_count = sum(value[1] for value in values)\n",
    "        average = total_sum / total_count\n",
    "        result[key] = average\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3.5, 'b': 5.0}\n"
     ]
    }
   ],
   "source": [
    "data = [('a', 4), ('b', 5), ('a', 3)]\n",
    "# Combine the functions to compute the average value for each key\n",
    "intermediate = map_function(data)\n",
    "grouped = shuffle_and_sort(intermediate)\n",
    "result = reduce_function(grouped)\n",
    "# Output the results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-process simulation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a distributed environment more realistically in Python,  i use multiprocessing to mimic the parallel processing that happens in a true MapReduce framework. \n",
    "\n",
    "- Data Splitting: The dataset is split into chunks, with each chunk processed by a separate worker. This simulates the distribution of data across different nodes in a distributed system.\n",
    "\n",
    "- Map Phase: Each worker applies the map_function to its chunk of data. This is done in parallel across all workers.\n",
    "\n",
    "- Shuffling and Combining: The results from all workers are collected into a single list. In a true distributed system, this would correspond to the shuffle and sort phase, where data with the same keys are moved to the same reducer.\n",
    "\n",
    "- Reduce Phase: The reduce_function processes the combined intermediate results to compute the final average for each key.\n",
    "\n",
    "- Multiprocessing Pool: The multiprocessing.Pool manages the distribution of work and collection of results across multiple processes, mimicking the behavior of a MapReduce framework's managing system.\n",
    "\n",
    "This approach, while still running on a single machine, better simulates the behavior of distributed systems by utilizing parallel processing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Multi-process Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(data_chunk):\n",
    "    \"\"\"Map function to process each chunk of data.\"\"\"\n",
    "    result = []\n",
    "    for key, value in data_chunk:\n",
    "        result.append((key, (value, 1)))\n",
    "    return result\n",
    "\n",
    "def reduce_function(intermediate_data):\n",
    "    \"\"\"Reduce function to compute the average from summed values.\"\"\"\n",
    "    totals = defaultdict(list)\n",
    "    for key, value in intermediate_data:\n",
    "        totals[key].append(value)\n",
    "    \n",
    "    final_result = {}\n",
    "    for key, values in totals.items():\n",
    "        total_sum = sum(val[0] for val in values)\n",
    "        total_count = sum(val[1] for val in values)\n",
    "        final_result[key] = total_sum / total_count\n",
    "    return final_result\n",
    "\n",
    "def distribute_computation(data, map_func, reduce_func, num_workers=None):\n",
    "    if num_workers is None:\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    num_workers = min(num_workers, len(data))\n",
    "    chunk_size = max(1, len(data) // num_workers)  \n",
    "    \n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        # Split the data into chunks\n",
    "        data_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "        \n",
    "        # Map phase: process each chunk in parallel\n",
    "        map_results = pool.map(map_func, data_chunks)\n",
    "        \n",
    "        # Combine all intermediate results for the reduce phase\n",
    "        combined_results = [item for sublist in map_results for item in sublist]\n",
    "    \n",
    "    # Reduce phase: compute final results\n",
    "    final_result = reduce_func(combined_results)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Related tool functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Function that writes data to a file\n",
    "This function generates a specified number of key-value pairs, one letter for each key, and a random number for each value, and appends these pairs to data.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def append_data_to_file(num_pairs, filename=\"data.txt\"):\n",
    "    \"\"\"Append specified number of key-value pairs to a file.\"\"\"\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        for _ in range(num_pairs):\n",
    "            key = random.choice(string.ascii_lowercase)  # Generate a random letter\n",
    "            value = random.randint(1, 100)  # Generate a random integer between 1 and 100\n",
    "            file.write(f\"{key},{value}\\n\")  # Write the key-value pair to the file\n",
    "\n",
    "append_data_to_file(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Functions that read data from files\n",
    "This function is used to read key-value pairs from data.txt and convert them to a format (list form) suitable for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(filename=\"data.txt\"):\n",
    "    \"\"\"Read key-value pairs from a file and return as a list of tuples.\"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            key, value = line.strip().split(',')\n",
    "            data.append((key, int(value)))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 A function that saves processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(results, filename=\"result.txt\"):\n",
    "    \"\"\"Save the results dictionary to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, value in results.items():\n",
    "            file.write(f\"{key},{value}\\n\")\n",
    "\n",
    "# save_results_to_file({'a': 25.5, 'b': 75})\n",
    "# # data = read_data_from_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute `python Multi-process.py` to get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'d': 43.5, 'e': 54.0, 'w': 47.0, 'o': 42.0, 'l': 35.5, 'b': 64.0, 's': 37.666666666666664, 'f': 78.66666666666667, 'q': 46.666666666666664, 'm': 42.333333333333336, 'c': 48.333333333333336, 'r': 67.5, 't': 44.25, 'i': 45.333333333333336, 'z': 19.5, 'j': 37.666666666666664, 'h': 33.0, 'x': 92.5, 'u': 32.5, 'v': 79.66666666666667, 'g': 48.0, 'k': 31.0, 'p': 7.0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmoney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
